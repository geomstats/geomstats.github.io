

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tutorial: Hyperbolic Embedding of Graphs &mdash; Geomstats latest documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="geomstats.github.io/05_embedding_graph_structured_data_h2.html" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Tutorial: Fréchet Mean and Tangent PCA" href="04_frechet_mean_and_tangent_pca.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Geomstats
          

          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="first-steps.html">First steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_data_on_manifolds.html">Tutorial: Data on Manifolds</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_from_vector_spaces_to_manifolds.html">Tutorial: From vector spaces to manifolds</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_simple_machine_learning_tangent_spaces.html">Tutorial: Learning on Tangent Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_frechet_mean_and_tangent_pca.html">Tutorial: Fréchet Mean and Tangent PCA</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial: Hyperbolic Embedding of Graphs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parameters-and-initialization">Parameters and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-function">Loss function.</a></li>
<li class="toctree-l2"><a class="reference internal" href="#riemannian-optimization">Riemannian optimization.</a></li>
<li class="toctree-l2"><a class="reference internal" href="#capturing-the-graph-structure">Capturing the graph structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#numerically-optimizing-the-loss-function">Numerically optimizing the loss function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#plotting-results">Plotting results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Geomstats</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tutorial: Hyperbolic Embedding of Graphs</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/05_embedding_graph_structured_data_h2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="tutorial-hyperbolic-embedding-of-graphs">
<h1>Tutorial: Hyperbolic Embedding of Graphs<a class="headerlink" href="#tutorial-hyperbolic-embedding-of-graphs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>From social networks to parse trees, knowledge graphs to protein
interaction networks, Graph-Structured Data is endemic to a wide variety
of natural and engineered systems. Often, understanding the structure
and/or dynamics of these graphs yields insight into the systems under
investigation. Take, for example, the problems of finding key
influencers or distinct communities within social networks.</p>
<p>The goal of graph embedding is to find a way of representing the graph
in a space which more readily lends itself to analysis/investigation.
One approach is to identify points in a vector space with nodes of the
graph in such a way that important relations between nodes are preserved
via relations between their corresponding points.</p>
<p>There are a wide variety of methods which approach this problem in
different ways and for different aims, say for clustering or for link
prediction. Recently, the embedding of Graph Structured Data (GSD) on
manifolds has received considerable attention. In particular, much work
has shown that hyperbolic spaces are beneficial for a wide variety of
tasks with GSD. This tutorial shows how to learn such embeddings using
the Poincaré Ball manifold and the well-known ‘Karate Club’ social
network dataset with <code class="docutils literal notranslate"><span class="pre">geomstats</span></code>. This data and several others can be
found in the <code class="docutils literal notranslate"><span class="pre">datasets.data</span></code> module of the project’s github
repository.</p>
<p><img alt="KarateEmbedding" src="figures/karate_embedding_iterations.gif" /> <em>Learning a Poincaré disk embedding of the Karate club
graph dataset</em></p>
</div>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<p>We start by importing standard tools for logging and visualization,
allowing us to draw the embedding of the GSD on the manifold. Next, we
import the manifold of interest, visualization tools, and other methods
from <code class="docutils literal notranslate"><span class="pre">geomstats</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">geomstats.backend</span> <span class="k">as</span> <span class="nn">gs</span>
<span class="kn">import</span> <span class="nn">geomstats.visualization</span> <span class="k">as</span> <span class="nn">visualization</span>

<span class="kn">from</span> <span class="nn">geomstats.datasets.utils</span> <span class="kn">import</span> <span class="n">load_karate_graph</span>
<span class="kn">from</span> <span class="nn">geomstats.geometry.poincare_ball</span> <span class="kn">import</span> <span class="n">PoincareBall</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INFO</span><span class="p">:</span> <span class="n">Using</span> <span class="n">numpy</span> <span class="n">backend</span>
</pre></div>
</div>
</div>
<div class="section" id="parameters-and-initialization">
<h2>Parameters and Initialization<a class="headerlink" href="#parameters-and-initialization" title="Permalink to this headline">¶</a></h2>
<p>We define the following parameters needed for embedding:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>random.seed</p></td>
<td><p>An initial manually set number
for generating pseudorandom
numbers</p></td>
</tr>
<tr class="row-odd"><td><p>dim</p></td>
<td><p>Dimensions of the manifold used
for embedding</p></td>
</tr>
<tr class="row-even"><td><p>max_epochs</p></td>
<td><p>Number of iterations for learning
the embedding</p></td>
</tr>
<tr class="row-odd"><td><p>lr</p></td>
<td><p>Learning rate</p></td>
</tr>
<tr class="row-even"><td><p>n_negative</p></td>
<td><p>Number of negative samples</p></td>
</tr>
<tr class="row-odd"><td><p>context_size</p></td>
<td><p>Size of the considered context
for each node of the graph</p></td>
</tr>
</tbody>
</table>
<p>Let us discuss a few things about the parameters of the above table. The
number of dimensions should be high (i.e., 10+) for large datasets
(i.e., where the number of nodes/edges is significantly large). In this
tutorial we consider a dataset that is quite small with only 34 nodes.
The Poincaré disk of only two dimensions is therefore sufficient to
capture the complexity of the graph and provide a faithful
representation. Some parameters are hard to know in advance, such as
<code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">lr</span></code>. These should be tuned specifically for each
dataset. Visualization can help with tuning the parameters. Also, one
can perform a grid search to find values of these parameters which
maximize some performance function. In learning embeddings, one can
consider performance metrics such as a measure for cluster seperability
or normalized mutual information (NMI) or others. Similarly, the number
of negative samples and context size can also be thought of as
hyperparameters and will be further discussed in the sequel. An instance
of the <code class="docutils literal notranslate"><span class="pre">Graph</span></code> class is created and set to the Karate club dataset.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gs</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">max_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lr</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">n_negative</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">karate_graph</span> <span class="o">=</span> <span class="n">load_karate_graph</span><span class="p">()</span>
</pre></div>
</div>
<p>The Zachary karate club network was collected from the members of a
university karate club by Wayne Zachary in 1977. Each node represents a
member of the club, and each edge represents an undirected relation
between two members. An often discussed problem using this dataset is to
find the two groups of people into which the karate club split after an
argument between two teachers. Some information about the dataset is
displayed to provide insight into its complexity.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_vertices_by_edges</span> <span class="o">=</span>\
    <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">e_2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">e_2</span> <span class="ow">in</span> <span class="n">karate_graph</span><span class="o">.</span><span class="n">edges</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Number of vertices: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">karate_graph</span><span class="o">.</span><span class="n">edges</span><span class="p">))</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
    <span class="s1">&#39;Mean edge-vertex ratio: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">nb_vertices_by_edges</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">karate_graph</span><span class="o">.</span><span class="n">edges</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INFO</span><span class="p">:</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">vertices</span><span class="p">:</span> <span class="mi">34</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">Mean</span> <span class="n">edge</span><span class="o">-</span><span class="n">vertex</span> <span class="n">ratio</span><span class="p">:</span> <span class="mf">4.588235294117647</span>
</pre></div>
</div>
<p>Denote <span class="math notranslate nohighlight">\(V\)</span> as the set of nodes and <span class="math notranslate nohighlight">\(E \subset V\times V\)</span> the
set of edges. The goal of embedding GSD is to provide a faithful and
exploitable representation of the graph structure. It is mainly achieved
by preserving <em>first-order</em> proximity that enforces nodes sharing edges
to be close to each other. It can additionally preserve <em>second-order</em>
proximity that enforces two nodes sharing the same context (i.e., nodes
that share neighbors but are not necessarily directly connected) to be
close. Let <span class="math notranslate nohighlight">\(\mathbb{B}^m\)</span> be the Poincaré Ball of dimension
<span class="math notranslate nohighlight">\(m\)</span> equipped with the distance function <span class="math notranslate nohighlight">\(d\)</span>. The below
figure shows geodesics between pairs of points on <span class="math notranslate nohighlight">\(\mathbb{B}^2\)</span>.
Geodesics are the shortest path between two points. The distance
function <span class="math notranslate nohighlight">\(d\)</span> of two points is the length of the geodesic that
links them.</p>
<p>Declaring an instance of the <code class="docutils literal notranslate"><span class="pre">PoincareBall</span></code> manifold of two dimensions
in <code class="docutils literal notranslate"><span class="pre">geomstats</span></code> is straightforward:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hyperbolic_manifold</span> <span class="o">=</span> <span class="n">PoincareBall</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p><em>first</em> and <em>second-order</em> proximities can be achieved by optimising the
following loss functions:</p>
</div>
<div class="section" id="loss-function">
<h2>Loss function.<a class="headerlink" href="#loss-function" title="Permalink to this headline">¶</a></h2>
<p>To preserve first and second-order proximities we adopt a loss function
similar to (Nickel, 2017) and consider the negative sampling approach as
in (Mikolov, 2013) :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = - \sum_{v_i\in V} \sum_{v_j \in C_i} \bigg[ log(\sigma(-d^2(\phi_i, \phi_j'))) + \sum_{v_k\sim \mathcal{P}_n} log(\sigma(d^2(\phi_i, \phi_k')))  \bigg]\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(x)=\frac{1}{1+e^{-x}}\)</span> is the sigmoid function and
<span class="math notranslate nohighlight">\(\phi_i \in \mathbb{B}^m\)</span> is the embedding of the <span class="math notranslate nohighlight">\(i\)</span>-th
node of <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(C_i\)</span> the nodes in the context of the
<span class="math notranslate nohighlight">\(i\)</span>-th node, <span class="math notranslate nohighlight">\(\phi_j'\in \mathbb{B}^m\)</span> the embedding of
<span class="math notranslate nohighlight">\(v_j\in C_i\)</span> and <span class="math notranslate nohighlight">\(\mathcal{P}_n\)</span> the negative sampling
distribution over <span class="math notranslate nohighlight">\(V\)</span>:
<span class="math notranslate nohighlight">\(\mathcal{P}_n(v)=\frac{deg(v)^{3/4}}{\sum_{v_i\in V}deg(v_i)^{3/4}}\)</span>.
Intuitively one can see that to minimizing <span class="math notranslate nohighlight">\(L\)</span>, the distance
between <span class="math notranslate nohighlight">\(v_i\)</span> and <span class="math notranslate nohighlight">\(v_j\)</span> should get smaller, while the one
between <span class="math notranslate nohighlight">\(v_i\)</span> and <span class="math notranslate nohighlight">\(v_k\)</span> would get larger.</p>
</div>
<div class="section" id="riemannian-optimization">
<h2>Riemannian optimization.<a class="headerlink" href="#riemannian-optimization" title="Permalink to this headline">¶</a></h2>
<p>Following the idea of (Ganea, 2018) we use the following formula to
optimize <span class="math notranslate nohighlight">\(L\)</span>:</p>
<div class="math notranslate nohighlight">
\[\phi^{t+1} = \text{Exp}_{\phi^t} \left( -lr \frac{\partial L}{\partial \phi} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> is a parameter of <span class="math notranslate nohighlight">\(L\)</span>,
<span class="math notranslate nohighlight">\(t\in\{1,2,\cdots\}\)</span> is the epoch iteration number and <span class="math notranslate nohighlight">\(lr\)</span>
is the learning rate. The formula consists of first computing the usual
gradient of the loss function giving the direction in which the
parameter should move. The Riemannian exponential map <span class="math notranslate nohighlight">\(\text{Exp}\)</span>
is a function that takes a base point <span class="math notranslate nohighlight">\(\phi^t\)</span> and some direction
vector <span class="math notranslate nohighlight">\(T\)</span> and returns the point <span class="math notranslate nohighlight">\(\phi^{t+1}\)</span> such that
<span class="math notranslate nohighlight">\(\phi^{t+1}\)</span> belongs to the geodesic initiated from
<span class="math notranslate nohighlight">\(\phi{t}\)</span> in the direction of <span class="math notranslate nohighlight">\(T\)</span> and the length of the
geoedesic curve between <span class="math notranslate nohighlight">\(\phi^t\)</span> and <span class="math notranslate nohighlight">\(\phi^{t+1}\)</span> is of 1
unit. The Riemannian exponential map is implemented as a method of the
<code class="docutils literal notranslate"><span class="pre">PoincareBallMetric</span></code> class in the <code class="docutils literal notranslate"><span class="pre">geometry</span></code> module of
<code class="docutils literal notranslate"><span class="pre">geomstats</span></code>.</p>
<p>Therefore to minimize <span class="math notranslate nohighlight">\(L\)</span> we will need to compute its gradient.
Several steps are required to do so, 1. Compute the gradient of the
squared distance 2. Compute the gradient of the log sigmoid 3. Compute
the gradient of the composision of 1. and 2.</p>
<p>For 1., we use the formula proposed by (Arnaudon, 2013) which uses the
Riemannian logarithmic map to compute the gradient of the distance. This
is implemented as</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grad_squared_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient of squared hyperbolic distance.</span>

<span class="sd">    Gradient of the squared distance based on the</span>
<span class="sd">    Ball representation according to point_a</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    point_a : array-like, shape=[n_samples, dim]</span>
<span class="sd">        First point in hyperbolic space.</span>
<span class="sd">    point_b : array-like, shape=[n_samples, dim]</span>
<span class="sd">        Second point in hyperbolic space.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist : array-like, shape=[n_samples, 1]</span>
<span class="sd">        Geodesic squared distance between the two points.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hyperbolic_metric</span> <span class="o">=</span> <span class="n">PoincareBall</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">metric</span>
    <span class="n">log_map</span> <span class="o">=</span> <span class="n">hyperbolic_metric</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">point_b</span><span class="p">,</span> <span class="n">point_a</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_map</span>
</pre></div>
</div>
<p>For 2. define the <code class="docutils literal notranslate"><span class="pre">log_sigmoid</span></code> corresponding as follows:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_sigmoid</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logsigmoid function.</span>

<span class="sd">    Apply log sigmoid function</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vector : array-like, shape=[n_samples, dim]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result : array-like, shape=[n_samples, dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gs</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">gs</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">vector</span><span class="p">))))</span>
</pre></div>
</div>
<p>The gradient of the logarithm of sigmoid function is implemented as:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grad_log_sigmoid</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient of log sigmoid function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vector : array-like, shape=[n_samples, dim]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    gradient : array-like, shape=[n_samples, dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">gs</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vector</span><span class="p">))</span>
</pre></div>
</div>
<p>For 3., apply the composition rule to obtain the gradient of <span class="math notranslate nohighlight">\(L\)</span>.
The following function given <span class="math notranslate nohighlight">\(\phi_i\)</span>, <span class="math notranslate nohighlight">\(\phi'_j\)</span> and
<span class="math notranslate nohighlight">\(\phi'_k\)</span> returns the total value of <span class="math notranslate nohighlight">\(L\)</span> and its gradient
vector at <span class="math notranslate nohighlight">\(\phi_i\)</span>. For the value of <span class="math notranslate nohighlight">\(L\)</span> the loss function
formula is simply applied. For the gradient, we apply the composition of
<code class="docutils literal notranslate"><span class="pre">grad_log_sigmoid</span></code> with <code class="docutils literal notranslate"><span class="pre">grad_squared_distance</span></code> while paying
attention to the signs.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">example_embedding</span><span class="p">,</span> <span class="n">context_embedding</span><span class="p">,</span> <span class="n">negative_embedding</span><span class="p">,</span>
         <span class="n">manifold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss and grad.</span>

<span class="sd">    Compute loss and grad given embedding of the current example,</span>
<span class="sd">    embedding of the context and negative sampling embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_edges</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span>\
        <span class="n">negative_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">example_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">example_embedding</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">example_embedding</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">context_embedding</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">context_embedding</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">positive_distance</span> <span class="o">=</span>\
        <span class="n">manifold</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">squared_dist</span><span class="p">(</span>
            <span class="n">example_embedding</span><span class="p">,</span> <span class="n">context_embedding</span><span class="p">)</span>
    <span class="n">positive_loss</span> <span class="o">=</span>\
        <span class="n">log_sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">positive_distance</span><span class="p">)</span>

    <span class="n">reshaped_example_embedding</span> <span class="o">=</span>\
        <span class="n">gs</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">example_embedding</span><span class="p">,</span> <span class="n">n_edges</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">negative_distance</span> <span class="o">=</span>\
        <span class="n">manifold</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">squared_dist</span><span class="p">(</span>
            <span class="n">reshaped_example_embedding</span><span class="p">,</span> <span class="n">negative_embedding</span><span class="p">)</span>
    <span class="n">negative_loss</span> <span class="o">=</span> <span class="n">log_sigmoid</span><span class="p">(</span><span class="n">negative_distance</span><span class="p">)</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">positive_loss</span> <span class="o">+</span> <span class="n">negative_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

    <span class="n">positive_log_sigmoid_grad</span> <span class="o">=</span>\
        <span class="o">-</span><span class="n">grad_log_sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">positive_distance</span><span class="p">)</span>

    <span class="n">positive_distance_grad</span> <span class="o">=</span>\
        <span class="n">grad_squared_distance</span><span class="p">(</span><span class="n">example_embedding</span><span class="p">,</span> <span class="n">context_embedding</span><span class="p">)</span>

    <span class="n">positive_grad</span> <span class="o">=</span>\
        <span class="n">gs</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">positive_log_sigmoid_grad</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>\
        <span class="o">*</span> <span class="n">positive_distance_grad</span>

    <span class="n">negative_distance_grad</span> <span class="o">=</span>\
        <span class="n">grad_squared_distance</span><span class="p">(</span><span class="n">reshaped_example_embedding</span><span class="p">,</span> <span class="n">negative_embedding</span><span class="p">)</span>

    <span class="n">negative_distance</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">(</span><span class="n">negative_distance</span><span class="p">,</span>
                                      <span class="n">to_ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">negative_log_sigmoid_grad</span> <span class="o">=</span>\
        <span class="n">grad_log_sigmoid</span><span class="p">(</span><span class="n">negative_distance</span><span class="p">)</span>

    <span class="n">negative_grad</span> <span class="o">=</span> <span class="n">negative_log_sigmoid_grad</span>\
        <span class="o">*</span> <span class="n">negative_distance_grad</span>
    <span class="n">example_grad</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">positive_grad</span> <span class="o">+</span> <span class="n">negative_grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">example_grad</span>
</pre></div>
</div>
</div>
<div class="section" id="capturing-the-graph-structure">
<h2>Capturing the graph structure<a class="headerlink" href="#capturing-the-graph-structure" title="Permalink to this headline">¶</a></h2>
<p>At this point we have the necessary bricks to compute the resulting
gradient of <span class="math notranslate nohighlight">\(L\)</span>. We are ready to prepare the nodes <span class="math notranslate nohighlight">\(v_i\)</span>,
<span class="math notranslate nohighlight">\(v_j\)</span> and <span class="math notranslate nohighlight">\(v_k\)</span> and initialise their embeddings
<span class="math notranslate nohighlight">\(\phi_i\)</span>, <span class="math notranslate nohighlight">\(\phi^{'}_j\)</span> and <span class="math notranslate nohighlight">\(\phi^{'}_k\)</span>. First,
initialize an array that will hold embeddings <span class="math notranslate nohighlight">\(\phi_i\)</span> of each
node <span class="math notranslate nohighlight">\(v_i\in V\)</span> with random points belonging to the Poincaré disk.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">karate_graph</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">*</span> <span class="mf">0.2</span>
</pre></div>
</div>
<p>Next, to prepare the context nodes <span class="math notranslate nohighlight">\(v_j\)</span> for each node
<span class="math notranslate nohighlight">\(v_i\)</span>, we compute random walks initialised from each <span class="math notranslate nohighlight">\(v_i\)</span>
up to some length (5 by default). The latter is done via a special
function within the <code class="docutils literal notranslate"><span class="pre">Graph</span></code> class. The nodes <span class="math notranslate nohighlight">\(v_j\)</span> will be later
picked from the random walk of <span class="math notranslate nohighlight">\(v_i\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_walks</span> <span class="o">=</span> <span class="n">karate_graph</span><span class="o">.</span><span class="n">random_walk</span><span class="p">()</span>
</pre></div>
</div>
<p>Negatively sampled nodes <span class="math notranslate nohighlight">\(v_k\)</span> are chosen according to the
previously defined probability distribution function
<span class="math notranslate nohighlight">\(\mathcal{P}_n(v_k)\)</span> implemented as</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">negative_table_parameter</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">negative_sampling_table</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">nb_v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nb_vertices_by_edges</span><span class="p">):</span>
    <span class="n">negative_sampling_table</span> <span class="o">+=</span>\
        <span class="p">([</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nb">int</span><span class="p">((</span><span class="n">nb_v</span><span class="o">**</span><span class="p">(</span><span class="mf">3.</span> <span class="o">/</span> <span class="mf">4.</span><span class="p">)))</span> <span class="o">*</span> <span class="n">negative_table_parameter</span><span class="p">)</span>

<span class="n">negative_sampling_table</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">negative_sampling_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="numerically-optimizing-the-loss-function">
<h2>Numerically optimizing the loss function<a class="headerlink" href="#numerically-optimizing-the-loss-function" title="Permalink to this headline">¶</a></h2>
<p>Optimising the loss function is performed numerically over the number of
epochs. At each iteration, we will compute the gradient of <span class="math notranslate nohighlight">\(L\)</span>.
Then the graph nodes are moved in the direction pointed by the gradient.
The movement of the nodes is performed by following geodesics in the
gradient direction. The key to obtain an embedding representing
accurately the dataset, is to move the nodes smoothly rather than brutal
movements. This is done by tuning the learning rate, such as at each
epoch all the nodes made small movements.</p>
<p>A <em>first level</em> loop iterates over the epochs, the table <code class="docutils literal notranslate"><span class="pre">total_loss</span></code>
will record the value of <span class="math notranslate nohighlight">\(L\)</span> at each iteration and help us track
the minimization of <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>A <em>second level</em> nested loop iterates over each path in the previously
computed random walks. Observing these walks, notice that nodes having
many edges appear more often. Such nodes can be considered as important
crossroads and will therefore be subject to a greater number of
embedding updates. This is one of the main reasons why random walks have
proven to be effective in capturing the structure of graphs. The context
of each <span class="math notranslate nohighlight">\(v_i\)</span> will be the set of nodes <span class="math notranslate nohighlight">\(v_j\)</span> belonging to
the random walk from <span class="math notranslate nohighlight">\(v_i\)</span>. The <code class="docutils literal notranslate"><span class="pre">context_size</span></code> specified earlier
will limit the length of the walk to be considered. Similarly, we use
the same <code class="docutils literal notranslate"><span class="pre">context_size</span></code> to limit the number of negative samples. We
find <span class="math notranslate nohighlight">\(\phi_i\)</span> from the <code class="docutils literal notranslate"><span class="pre">embeddings</span></code> array.</p>
<p>A <em>third level</em> nested loop will iterate on each <span class="math notranslate nohighlight">\(v_j\)</span> and
<span class="math notranslate nohighlight">\(v_k\)</span>. From within, we find <span class="math notranslate nohighlight">\(\phi'_j\)</span> and <span class="math notranslate nohighlight">\(\phi'_k\)</span>
then call the <code class="docutils literal notranslate"><span class="pre">loss</span></code> function to compute the gradient. Then the
Riemannian exponential map is applied to find the new value of
<span class="math notranslate nohighlight">\(\phi_i\)</span> as we mentioned before.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">random_walks</span><span class="p">:</span>

        <span class="k">for</span> <span class="n">example_index</span><span class="p">,</span> <span class="n">one_path</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
            <span class="n">context_index</span> <span class="o">=</span> <span class="n">path</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">example_index</span> <span class="o">-</span> <span class="n">context_size</span><span class="p">):</span>
                                 <span class="nb">min</span><span class="p">(</span><span class="n">example_index</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">))]</span>
            <span class="n">negative_index</span> <span class="o">=</span>\
                <span class="n">gs</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">negative_sampling_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                  <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">context_index</span><span class="p">),</span>
                                  <span class="n">n_negative</span><span class="p">))</span>
            <span class="n">negative_index</span> <span class="o">=</span> <span class="n">negative_sampling_table</span><span class="p">[</span><span class="n">negative_index</span><span class="p">]</span>

            <span class="n">example_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">one_path</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">one_context_i</span><span class="p">,</span> <span class="n">one_negative_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">context_index</span><span class="p">,</span>
                                                     <span class="n">negative_index</span><span class="p">):</span>
                <span class="n">context_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">one_context_i</span><span class="p">]</span>
                <span class="n">negative_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">one_negative_i</span><span class="p">]</span>
                <span class="n">l</span><span class="p">,</span> <span class="n">g_ex</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span>
                    <span class="n">example_embedding</span><span class="p">,</span>
                    <span class="n">context_embedding</span><span class="p">,</span>
                    <span class="n">negative_embedding</span><span class="p">,</span>
                    <span class="n">hyperbolic_manifold</span><span class="p">)</span>
                <span class="n">total_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

                <span class="n">example_to_update</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">one_path</span><span class="p">]</span>
                <span class="n">embeddings</span><span class="p">[</span><span class="n">one_path</span><span class="p">]</span> <span class="o">=</span> <span class="n">hyperbolic_manifold</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                    <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">g_ex</span><span class="p">,</span> <span class="n">example_to_update</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s1">&#39;iteration </span><span class="si">%d</span><span class="s1"> loss_value </span><span class="si">%f</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">total_loss</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">total_loss</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">0</span> <span class="n">loss_value</span> <span class="mf">1.826876</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">1</span> <span class="n">loss_value</span> <span class="mf">1.774560</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">2</span> <span class="n">loss_value</span> <span class="mf">1.725700</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">3</span> <span class="n">loss_value</span> <span class="mf">1.663358</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">4</span> <span class="n">loss_value</span> <span class="mf">1.655706</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">5</span> <span class="n">loss_value</span> <span class="mf">1.615405</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">6</span> <span class="n">loss_value</span> <span class="mf">1.581097</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">7</span> <span class="n">loss_value</span> <span class="mf">1.526418</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">8</span> <span class="n">loss_value</span> <span class="mf">1.507913</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">9</span> <span class="n">loss_value</span> <span class="mf">1.505934</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">10</span> <span class="n">loss_value</span> <span class="mf">1.466526</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">11</span> <span class="n">loss_value</span> <span class="mf">1.453769</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">12</span> <span class="n">loss_value</span> <span class="mf">1.443878</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">13</span> <span class="n">loss_value</span> <span class="mf">1.451272</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">14</span> <span class="n">loss_value</span> <span class="mf">1.397864</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">15</span> <span class="n">loss_value</span> <span class="mf">1.396170</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">16</span> <span class="n">loss_value</span> <span class="mf">1.373677</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">17</span> <span class="n">loss_value</span> <span class="mf">1.390120</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">18</span> <span class="n">loss_value</span> <span class="mf">1.382397</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">19</span> <span class="n">loss_value</span> <span class="mf">1.404103</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">20</span> <span class="n">loss_value</span> <span class="mf">1.395782</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">21</span> <span class="n">loss_value</span> <span class="mf">1.389617</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">22</span> <span class="n">loss_value</span> <span class="mf">1.410152</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">23</span> <span class="n">loss_value</span> <span class="mf">1.390600</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">24</span> <span class="n">loss_value</span> <span class="mf">1.374832</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">25</span> <span class="n">loss_value</span> <span class="mf">1.367194</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">26</span> <span class="n">loss_value</span> <span class="mf">1.323190</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">27</span> <span class="n">loss_value</span> <span class="mf">1.389616</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">28</span> <span class="n">loss_value</span> <span class="mf">1.361034</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">29</span> <span class="n">loss_value</span> <span class="mf">1.384930</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">30</span> <span class="n">loss_value</span> <span class="mf">1.340814</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">31</span> <span class="n">loss_value</span> <span class="mf">1.349682</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">32</span> <span class="n">loss_value</span> <span class="mf">1.317423</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">33</span> <span class="n">loss_value</span> <span class="mf">1.346869</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">34</span> <span class="n">loss_value</span> <span class="mf">1.327198</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">35</span> <span class="n">loss_value</span> <span class="mf">1.363809</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">36</span> <span class="n">loss_value</span> <span class="mf">1.352347</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">37</span> <span class="n">loss_value</span> <span class="mf">1.317670</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">38</span> <span class="n">loss_value</span> <span class="mf">1.320039</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">39</span> <span class="n">loss_value</span> <span class="mf">1.323888</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">40</span> <span class="n">loss_value</span> <span class="mf">1.341444</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">41</span> <span class="n">loss_value</span> <span class="mf">1.312259</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">42</span> <span class="n">loss_value</span> <span class="mf">1.315983</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">43</span> <span class="n">loss_value</span> <span class="mf">1.305483</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">44</span> <span class="n">loss_value</span> <span class="mf">1.325384</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">45</span> <span class="n">loss_value</span> <span class="mf">1.328024</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">46</span> <span class="n">loss_value</span> <span class="mf">1.306958</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">47</span> <span class="n">loss_value</span> <span class="mf">1.303357</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">48</span> <span class="n">loss_value</span> <span class="mf">1.303790</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">49</span> <span class="n">loss_value</span> <span class="mf">1.324749</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">50</span> <span class="n">loss_value</span> <span class="mf">1.328376</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">51</span> <span class="n">loss_value</span> <span class="mf">1.313816</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">52</span> <span class="n">loss_value</span> <span class="mf">1.325978</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">53</span> <span class="n">loss_value</span> <span class="mf">1.317516</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">54</span> <span class="n">loss_value</span> <span class="mf">1.353495</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">55</span> <span class="n">loss_value</span> <span class="mf">1.331988</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">56</span> <span class="n">loss_value</span> <span class="mf">1.346874</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">57</span> <span class="n">loss_value</span> <span class="mf">1.348946</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">58</span> <span class="n">loss_value</span> <span class="mf">1.324719</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">59</span> <span class="n">loss_value</span> <span class="mf">1.330355</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">60</span> <span class="n">loss_value</span> <span class="mf">1.331077</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">61</span> <span class="n">loss_value</span> <span class="mf">1.305729</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">62</span> <span class="n">loss_value</span> <span class="mf">1.311746</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">63</span> <span class="n">loss_value</span> <span class="mf">1.347637</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">64</span> <span class="n">loss_value</span> <span class="mf">1.326300</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">65</span> <span class="n">loss_value</span> <span class="mf">1.309570</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">66</span> <span class="n">loss_value</span> <span class="mf">1.313999</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">67</span> <span class="n">loss_value</span> <span class="mf">1.346287</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">68</span> <span class="n">loss_value</span> <span class="mf">1.300901</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">69</span> <span class="n">loss_value</span> <span class="mf">1.323723</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">70</span> <span class="n">loss_value</span> <span class="mf">1.320784</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">71</span> <span class="n">loss_value</span> <span class="mf">1.313709</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">72</span> <span class="n">loss_value</span> <span class="mf">1.312143</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">73</span> <span class="n">loss_value</span> <span class="mf">1.309172</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">74</span> <span class="n">loss_value</span> <span class="mf">1.320642</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">75</span> <span class="n">loss_value</span> <span class="mf">1.308333</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">76</span> <span class="n">loss_value</span> <span class="mf">1.325884</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">77</span> <span class="n">loss_value</span> <span class="mf">1.316740</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">78</span> <span class="n">loss_value</span> <span class="mf">1.325933</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">79</span> <span class="n">loss_value</span> <span class="mf">1.316672</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">80</span> <span class="n">loss_value</span> <span class="mf">1.312291</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">81</span> <span class="n">loss_value</span> <span class="mf">1.332372</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">82</span> <span class="n">loss_value</span> <span class="mf">1.317499</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">83</span> <span class="n">loss_value</span> <span class="mf">1.329194</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">84</span> <span class="n">loss_value</span> <span class="mf">1.305926</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">85</span> <span class="n">loss_value</span> <span class="mf">1.304747</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">86</span> <span class="n">loss_value</span> <span class="mf">1.342343</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">87</span> <span class="n">loss_value</span> <span class="mf">1.331992</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">88</span> <span class="n">loss_value</span> <span class="mf">1.295439</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">89</span> <span class="n">loss_value</span> <span class="mf">1.332853</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">90</span> <span class="n">loss_value</span> <span class="mf">1.332004</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">91</span> <span class="n">loss_value</span> <span class="mf">1.357248</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">92</span> <span class="n">loss_value</span> <span class="mf">1.342234</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">93</span> <span class="n">loss_value</span> <span class="mf">1.329379</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">94</span> <span class="n">loss_value</span> <span class="mf">1.313617</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">95</span> <span class="n">loss_value</span> <span class="mf">1.310320</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">96</span> <span class="n">loss_value</span> <span class="mf">1.320590</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">97</span> <span class="n">loss_value</span> <span class="mf">1.315822</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">98</span> <span class="n">loss_value</span> <span class="mf">1.328819</span>
<span class="n">INFO</span><span class="p">:</span> <span class="n">iteration</span> <span class="mi">99</span> <span class="n">loss_value</span> <span class="mf">1.339718</span>
</pre></div>
</div>
</div>
<div class="section" id="plotting-results">
<h2>Plotting results<a class="headerlink" href="#plotting-results" title="Permalink to this headline">¶</a></h2>
<p>Once the <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> iterations of epochs is achieved, we can plot
the resulting <code class="docutils literal notranslate"><span class="pre">embeddings</span></code> array and the true labels shown as two
colors. At 100 epochs we can see that the two group of nodes with
different labels are moving away from each other on the manifold. If one
increases the <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code>, then further separability is achieved.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;r&#39;</span><span class="p">}</span>
<span class="n">group_1</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Group 1&#39;</span><span class="p">)</span>
<span class="n">group_2</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Group 2&#39;</span><span class="p">)</span>

<span class="n">circle</span> <span class="o">=</span> <span class="n">visualization</span><span class="o">.</span><span class="n">PoincareDisk</span><span class="p">(</span><span class="n">point_type</span><span class="o">=</span><span class="s1">&#39;ball&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">circle</span><span class="o">.</span><span class="n">set_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">circle</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i_embedding</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pt_id</span> <span class="o">=</span> <span class="n">i_embedding</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">karate_graph</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">pt_id</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">150</span>
        <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">pt_id</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span>
<span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Poincare Ball Embedding of the Karate Club Network&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="p">[</span><span class="n">group_1</span><span class="p">,</span> <span class="n">group_2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="_images/05_embedding_graph_structured_data_h2_36_0.png" src="_images/05_embedding_graph_structured_data_h2_36_0.png" />
<p>In <code class="docutils literal notranslate"><span class="pre">geomstats</span></code>, several unsupervized clustering algorithms on
manifolds are implemented such as <span class="math notranslate nohighlight">\(K\)</span>-means and
Expectation-Maximization.</p>
<p>Let us apply <span class="math notranslate nohighlight">\(K\)</span>-means to learn the node belonging of the two
groups and see how well we predicted the true labels. Lets first import
<span class="math notranslate nohighlight">\(K\)</span>-means</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">geomstats.learning.kmeans</span> <span class="kn">import</span> <span class="n">RiemannianKMeans</span>
</pre></div>
</div>
<p>Set the number of groups to 2.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Initialize an instance of <span class="math notranslate nohighlight">\(K\)</span>-means.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">RiemannianKMeans</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span> <span class="n">hyperbolic_manifold</span><span class="o">.</span><span class="n">metric</span><span class="p">,</span>
                          <span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span>
                          <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
                          <span class="n">mean_method</span><span class="o">=</span><span class="s1">&#39;frechet-poincare-ball&#39;</span>
                              <span class="p">)</span>
</pre></div>
</div>
<p>Fit the embedded nodes</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">centroids</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
<p>And plot the resulting labels provided by <span class="math notranslate nohighlight">\(K\)</span>-means</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">]</span>
<span class="n">circle</span> <span class="o">=</span> <span class="n">visualization</span><span class="o">.</span><span class="n">PoincareDisk</span><span class="p">(</span><span class="n">point_type</span><span class="o">=</span><span class="s1">&#39;ball&#39;</span><span class="p">)</span>
<span class="n">fig2</span><span class="p">,</span> <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">circle</span><span class="o">.</span><span class="n">set_ax</span><span class="p">(</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">circle</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">group_1_predicted</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Group 1&#39;</span><span class="p">)</span>
<span class="n">group_2_predicted</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Group 2&#39;</span><span class="p">)</span>
<span class="n">group_centroids</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cluster centroids&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i_embedding</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pt_id</span> <span class="o">=</span> <span class="n">i_embedding</span>
        <span class="k">if</span> <span class="n">labels</span><span class="p">[</span><span class="n">i_embedding</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
            <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">150</span>
            <span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">pt_id</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i_centroid</span><span class="p">,</span> <span class="n">centroid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">centroids</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">centroid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">centroid</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span>
        <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;K-means applied to Karate club embedding&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span> <span class="o">=</span> <span class="p">[</span><span class="n">group_1_predicted</span><span class="p">,</span> <span class="n">group_2_predicted</span><span class="p">,</span> <span class="n">group_centroids</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="_images/05_embedding_graph_structured_data_h2_46_0.png" src="_images/05_embedding_graph_structured_data_h2_46_0.png" />
<p>By comparing the <span class="math notranslate nohighlight">\(K\)</span>-means labels and the true labels, notice how
<span class="math notranslate nohighlight">\(K\)</span>-means accurately finds the two groups of nodes (not perfectly,
e.g., nodes 2 and 8). We therefore achieved good performances in
predicting the belonging of each member of the Karate club to one of the
two groups.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>(Arnaudon, 2013) Arnaudon, Marc, Frédéric Barbaresco, and Le Yang.
“Riemannian medians and means with applications to radar signal
processing.” IEEE Journal of Selected Topics in Signal Processing 7.4
(2013): 595-604.</p>
<p>(Ganea, 2018) Ganea, Octavian, Gary Bécigneul, and Thomas Hofmann.
“Hyperbolic neural networks.” Advances in neural information processing
systems. 2018.</p>
<p>(Mikolov, 2013) Mikolov, Tomas, et al. “Distributed representations of
words and phrases and their compositionality.” Advances in neural
information processing systems. 2013.</p>
<p>(Nickel, 2017) Nickel, Maximillian, and Douwe Kiela. “Poincaré
embeddings for learning hierarchical representations.” Advances in
neural information processing systems. 2017.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="04_frechet_mean_and_tangent_pca.html" class="btn btn-neutral float-left" title="Tutorial: Fréchet Mean and Tangent PCA" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2019-2020, Geomstats, Inc..
      <span class="lastupdated">
        Last updated on Mar 18, 2021, 2:23:27 PM.
      </span>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>